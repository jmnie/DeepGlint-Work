{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First Part: Load the data: \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "def normalization(x_):\n",
    "    length = len(x_)\n",
    "    max_ = np.amax(x_)\n",
    "    min_ = np.min(x_)\n",
    "    x_ = 2*(x_ - min_)/(max_ - min_)\n",
    "    return x_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_train,y_train,x_test,y_test,x_vali,y_vali = ReadData_fer()\n",
    "\n",
    "# # # Normalization\n",
    "# # #x_train = normalization(x_train)\n",
    "# # #x_test = normalization(x_test)\n",
    "# # #x_vali = normalization(x_vali)\n",
    "\n",
    "# # x_train = x_train.reshape((len(x_train), 48, 48, 1))\n",
    "# # x_test = x_test.reshape((len(x_test), 48, 48, 1))\n",
    "# # x_vali = x_vali.reshape((len(x_vali),48,48,1))\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "# NB_CLASS = 7\n",
    "# x_train = scipy.io.loadmat('/train/execute/fer2013/x_train.mat')['x_train']\n",
    "# x_test = scipy.io.loadmat('/train/execute/fer2013/x_test.mat')['x_test']\n",
    "# x_vali = scipy.io.loadmat('/train/execute/fer2013/x_vali.mat')['x_vali']\n",
    "# y_train = np.loadtxt('/train/execute/fer2013/y_train.txt')\n",
    "# y_test = np.loadtxt('/train/execute/fer2013/y_test.txt')\n",
    "# y_vali = np.loadtxt('/train/execute/fer2013/y_vali.txt')\n",
    "\n",
    "# # Normalization\n",
    "# x_train = normalization(x_train)\n",
    "# x_test = normalization(x_test)\n",
    "# x_vali = normalization(x_vali)\n",
    "\n",
    "# x_train = x_train.reshape((len(x_train), 48, 48, 1))\n",
    "# x_test = x_test.reshape((len(x_test), 48, 48, 1))\n",
    "# x_vali = x_vali.reshape((len(x_vali),48,48,1))\n",
    "\n",
    "# y_train = to_categorical(y_train, NB_CLASS)\n",
    "# y_test = to_categorical(y_test, NB_CLASS)\n",
    "# y_vali = to_categorical(y_vali, NB_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and Vali Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(x_):\n",
    "    length = len(x_)\n",
    "    max_ = np.amax(x_)\n",
    "    min_ = np.min(x_)\n",
    "    x_ = 2*(x_ - min_)/(max_ - min_)\n",
    "    return x_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8, keras model implementation \n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, concatenate, \\\n",
    "    Activation, ZeroPadding2D\n",
    "from keras.layers import add, Flatten\n",
    "from keras.utils import plot_model\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import keras.backend.tensorflow_backend as KTF  \n",
    "KTF.set_session(tf.Session(config=tf.ConfigProto(device_count={'gpu':0})))  \n",
    "\n",
    "#THEANO_FLAGS=device=gpu, floatX=32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Constants\n",
    "NB_CLASS=8\n",
    "IM_WIDTH=224\n",
    "IM_HEIGHT=224\n",
    "CHANNEL = 3\n",
    "batch_size=16\n",
    "EPOCH=20\n",
    "\n",
    "\n",
    "def Conv2d_BN(x, nb_filter, kernel_size, strides=(1, 1), padding='same', name=None):\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "\n",
    "    #x = Conv2D(nb_filter, kernel_size, padding=padding, strides=strides, activation='relu', name=conv_name)(x)\n",
    "    x = Conv2D(nb_filter, kernel_size, padding=padding, strides=strides, name=conv_name)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization(axis=3, name=bn_name)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def identity_Block(inpt, nb_filter, kernel_size, strides=(1, 1), with_conv_shortcut=False):\n",
    "    x = Conv2d_BN(inpt, nb_filter=nb_filter, kernel_size=kernel_size, strides=strides, padding='same')\n",
    "    x = Conv2d_BN(x, nb_filter=nb_filter, kernel_size=kernel_size, padding='same')\n",
    "    if with_conv_shortcut:\n",
    "        shortcut = Conv2d_BN(inpt, nb_filter=nb_filter, strides=strides, kernel_size=kernel_size)\n",
    "        x = add([x, shortcut])\n",
    "        return x\n",
    "    else:\n",
    "        x = add([x, inpt])\n",
    "        return x\n",
    "\n",
    "def bottleneck_Block(inpt,nb_filters,strides=(1,1),with_conv_shortcut=False):\n",
    "    k1,k2,k3=nb_filters\n",
    "    x = Conv2d_BN(inpt, nb_filter=k1, kernel_size=1, strides=strides, padding='same')\n",
    "    x = Conv2d_BN(x, nb_filter=k2, kernel_size=3, padding='same')\n",
    "    x = Conv2d_BN(x, nb_filter=k3, kernel_size=1, padding='same')\n",
    "    if with_conv_shortcut:\n",
    "        shortcut = Conv2d_BN(inpt, nb_filter=k3, strides=strides, kernel_size=1)\n",
    "        x = add([x, shortcut])\n",
    "        return x\n",
    "    else:\n",
    "        x = add([x, inpt])\n",
    "        return x\n",
    "\n",
    "def resnet_34(width,height,channel,classes):\n",
    "    inpt = Input(shape=(width, height, channel))\n",
    "    x = ZeroPadding2D((3, 3))(inpt)\n",
    "    \n",
    "    ## channel 全部砍一半\n",
    "    #conv1\n",
    "    x = Conv2d_BN(x, nb_filter=32, kernel_size=(7, 7), strides=(2, 2), padding='valid')\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    #conv2_x\n",
    "    x = identity_Block(x, nb_filter=32, kernel_size=(3, 3))\n",
    "    x = identity_Block(x, nb_filter=32, kernel_size=(3, 3))\n",
    "    x = identity_Block(x, nb_filter=32, kernel_size=(3, 3))\n",
    "\n",
    "    #conv3_x\n",
    "    x = identity_Block(x, nb_filter=64, kernel_size=(3, 3), strides=(2, 2), with_conv_shortcut=True)\n",
    "    x = identity_Block(x, nb_filter=64, kernel_size=(3, 3))\n",
    "    x = identity_Block(x, nb_filter=64, kernel_size=(3, 3))\n",
    "    x = identity_Block(x, nb_filter=64, kernel_size=(3, 3))\n",
    "\n",
    "    #conv4_x\n",
    "    x = identity_Block(x, nb_filter=128, kernel_size=(3, 3), strides=(2, 2), with_conv_shortcut=True)\n",
    "    x = identity_Block(x, nb_filter=128, kernel_size=(3, 3))\n",
    "    x = identity_Block(x, nb_filter=128, kernel_size=(3, 3))\n",
    "    x = identity_Block(x, nb_filter=128, kernel_size=(3, 3))\n",
    "    x = identity_Block(x, nb_filter=128, kernel_size=(3, 3))\n",
    "    x = identity_Block(x, nb_filter=128, kernel_size=(3, 3))\n",
    "\n",
    "    #conv5_x\n",
    "    x = identity_Block(x, nb_filter=256, kernel_size=(3, 3), strides=(2, 2), with_conv_shortcut=True)\n",
    "    x = identity_Block(x, nb_filter=256, kernel_size=(3, 3))\n",
    "    x = identity_Block(x, nb_filter=256, kernel_size=(3, 3))\n",
    "    #x = AveragePooling2D(pool_size=(7, 7))(x)\n",
    "    #x = AveragePooling2D(pool_size=(2,2))(x)\n",
    "    x = AveragePooling2D(pool_size=(2,2))(x)\n",
    "    #print(x.shape)\n",
    "    x = Flatten()(x)\n",
    "    #print(\"After Flatten\",x.shape)\n",
    "    x = Dense(classes, activation='softmax')(x)\n",
    "    #print(\"Outputs shape\",x)\n",
    "    #print(\"Input shape\", inpt)\n",
    "    model = Model(inputs=inpt, outputs=x)\n",
    "    return model\n",
    "\n",
    "def acc_top2(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "\n",
    "def check_print():\n",
    "    # Create a Keras Model\n",
    "    model = resnet_34(IM_WIDTH,IM_HEIGHT,CHANNEL,NB_CLASS)\n",
    "    #model.summary()\n",
    "    # Save a PNG of the Model Build\n",
    "    #plot_model(model, to_file='resnet.png')\n",
    "    adam = keras.optimizers.Adam(lr=0.1)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc',top_k_categorical_accuracy])\n",
    "    print('Model Compiled')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadMapping():\n",
    "    import numpy as np\n",
    "    import json\n",
    "\n",
    "    def LoadJson(path):\n",
    "        with open(path) as fp:\n",
    "            result = json.load(fp)    \n",
    "\n",
    "        return result\n",
    "    \n",
    "    mapping_dir = '/train/execute/AffectNet/mapping'\n",
    "    y_train_path = mapping_dir + '/y_train.txt'\n",
    "    y_test_path = mapping_dir + '/y_test.txt'\n",
    "    y_vali_path = mapping_dir + '/y_vali.txt'\n",
    "    y_train = np.loadtxt(y_train_path)\n",
    "    y_test = np.loadtxt(y_test_path)\n",
    "    y_vali = np.loadtxt(y_vali_path)\n",
    "    x_train = LoadJson(mapping_dir + '/x_train.json')\n",
    "    x_test = LoadJson(mapping_dir + '/x_test.json')\n",
    "    x_vali = LoadJson(mapping_dir + '/x_vali.json')\n",
    "    \n",
    "    return x_train,y_train,x_test,y_test,x_vali,y_vali\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, name,batch_size=16, dim=(224,224), n_channels=3,\n",
    "                 n_classes=8, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.name = name\n",
    "        #self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.aug_gen = ImageDataGenerator(\n",
    "                      rotation_range=5,\n",
    "                      width_shift_range=0.2,\n",
    "                      height_shift_range=0.2,\n",
    "                      horizontal_flip=True)\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def __aug_(self,X,y):\n",
    "        \n",
    "        self.aug_gen.fit(X)\n",
    "\n",
    "        # fits the model on batches with real-time data augmentation:\n",
    "        return self.aug_gen.flow(X, y, batch_size=self.batch_size).next()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.dim[0], self.dim[1], self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        x_train_map,y_train_map,x_test_map,y_test_map,x_vali_map,y_vali_map = LoadMapping()\n",
    "        \n",
    "        #print(len(list_IDs_temp))\n",
    "        path = '/train/execute/AffectNet/Data/aligned/'\n",
    "        \n",
    "        for i in range(len(list_IDs_temp)):\n",
    "            index = list_IDs_temp[i]\n",
    "            key = x_train_map[index]#.encode('ascii')\n",
    "            X[i] = ReadImage(path,key)\n",
    "            y[i] = y_train_map[index] \n",
    "\n",
    "#         if self.name == 'train':\n",
    "#             for i in range(len(list_IDs_temp)):\n",
    "#                 index = list_IDs_temp[i]\n",
    "#                 key = x_train_map[index]#.encode('ascii')\n",
    "#                 X[i] = ReadImage(path,key)\n",
    "#                 y[i] = y_train_map[index] \n",
    "#         else:\n",
    "#             for i in range(len(list_IDs_temp)):\n",
    "#                 index = list_IDs_temp[i]\n",
    "#                 key = x_vali_map[index]#.encode('ascii')\n",
    "#                 X[i] = ReadImage(path,key)\n",
    "#                 y[i] = y_vali_map[index]\n",
    "\n",
    "\n",
    "        return self.__aug_(X,keras.utils.to_categorical(y, num_classes=self.n_classes))\n",
    "\n",
    "def ReadImage(path,key):\n",
    "    \n",
    "    file_path = path + key\n",
    "    im_ = Image.open(file_path)\n",
    "        ### Center Crop\n",
    "    x = 113 - 14\n",
    "    y = 130 - 30\n",
    "    length = 270\n",
    "    area = (x, y, x+length, y+length)\n",
    "    im_ = im_.crop(area)\n",
    "    im_ = (np.array(im_))\n",
    "    im_ = im_ * (1. / 255) - 0.5\n",
    "    im_ = RandomCrop(im_,224)\n",
    "    return im_\n",
    "\n",
    "def ReadImage_2(path,key):\n",
    "    \n",
    "    file_path = path + key\n",
    "    im_ = Image.open(file_path)\n",
    "    ### Center Crop\n",
    "    x = 113 - 14\n",
    "    y = 130 - 30\n",
    "    length = 224\n",
    "    area = (x, y, x+length, y+length)\n",
    "    im_ = im_.crop(area)\n",
    "    im_ = (np.array(im_)) \n",
    "    im_ = im_ * (1. / 255) - 0.5\n",
    "\n",
    "    return im_\n",
    "\n",
    "def RandomCrop(img,random_crop_size):\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy = random_crop_size\n",
    "    dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "def generateListID(length):\n",
    "    \n",
    "    result = []\n",
    "    for i in range(length):\n",
    "        result.append(i)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read Test Data:\n",
    "import numpy as np\n",
    "def ReadTest():\n",
    "    mapping_dir = '/train/execute/AffectNet/mapping'\n",
    "    path = '/train/execute/AffectNet/Data/aligned/'\n",
    "    \n",
    "    y_test = np.loadtxt(mapping_dir + '/y_test.txt')\n",
    "    y_test = to_categorical(y_test, num_classes = NB_CLASS)\n",
    "    with open(mapping_dir + '/x_test.json','r') as fp:\n",
    "        x_test_mapping = json.load(fp)\n",
    "        \n",
    "    X = np.empty((len(y_test),224,224,3))\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        key = x_test_mapping[i] \n",
    "        X[i] = ReadImage_2(path,key)\n",
    "        \n",
    "    return X, y_test\n",
    "\n",
    "def ReadVali():\n",
    "    mapping_dir = '/train/execute/AffectNet/mapping'\n",
    "    path = '/train/execute/AffectNet/Data/aligned/'\n",
    "    \n",
    "    y_ = np.loadtxt(mapping_dir + '/y_vali.txt')\n",
    "    y_ = to_categorical(y_, num_classes = NB_CLASS)\n",
    "    with open(mapping_dir + '/x_vali.json','r') as fp:\n",
    "        x_vali_mapping = json.load(fp)\n",
    "        \n",
    "    X = np.empty((len(y_),224,224,3))\n",
    "    \n",
    "    for i in range(len(y_)):\n",
    "        key = x_vali_mapping[i] \n",
    "        X[i] = ReadImage_2(path,key)\n",
    "        \n",
    "    return X, y_\n",
    "\n",
    "\n",
    "# x_test,y_test = ReadTest()\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot(result):\n",
    "    plt.style.use('ggplot')\n",
    "    ax1 = plt.subplot2grid((2, 2), (0, 0))\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax2 = plt.subplot2grid((2, 2), (0, 1))\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    #ax3 = plt.subplot2grid((2, 2), (0, 1), rowspan=2)\n",
    "    #ax3.set_title('Time')\n",
    "    #ax3.set_ylabel('Seconds')\n",
    "    \n",
    "    for result in results:\n",
    "        ax1.plot(result[0].epoch, result[0].history['val_acc'], label='Vali')\n",
    "        ax1.plot(result[0].epoch, result[0].history['acc'], label='Train')\n",
    "        ax2.plot(result[0].epoch, result[0].history['val_loss'], label='Vali')\n",
    "        ax2.plot(result[0].epoch, result[0].history['loss'], label='Train')\n",
    "\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    #ax3.bar(np.arange(len(results)), [x[1] for x in results],\n",
    "    #        align='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5078543675813063194\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 235929600\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 10041784300238997736\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\"\n",
      "]\n",
      "('Vali Shape', (2871, 224, 224, 3))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "      \n",
    "      from tensorflow.python.client import device_lib\n",
    "      print (device_lib.list_local_devices())\n",
    "\n",
    "      mapping_dir = '/train/execute/AffectNet/mapping'\n",
    "      y_train_path = mapping_dir + '/y_train.txt'\n",
    "      #y_vali_path = mapping_dir + '/y_vali.txt'\n",
    "      \n",
    "      y_train = np.loadtxt(y_train_path)\n",
    "      #y_vali = np.loadtxt(y_vali_path)\n",
    "      \n",
    "      x_vali,y_vali = ReadVali()\n",
    "      print(\"Vali Shape\",x_vali.shape)\n",
    "      x_test,y_test = ReadTest()\n",
    "      print(x_test.shape)\n",
    "      \n",
    "#       with open(mapping_dir + '/x_vali.json','r') as fp:\n",
    "#         x_vali_mapping = json.load(fp)\n",
    "        \n",
    "#       for i in range(len(x_vali)):\n",
    "#           print(x_vali_mapping[i])\n",
    "#           print(\"i: \",i,x_vali[i])\n",
    "      ## \n",
    "      listID_train = generateListID(len(y_train))\n",
    "      #listID_vali = generateListID(len(y_vali))\n",
    "   \n",
    "      \n",
    "      params = {'dim': (224,224),\n",
    "          'batch_size': batch_size,\n",
    "          'n_classes': NB_CLASS,\n",
    "          'n_channels': CHANNEL,\n",
    "          'shuffle': True}\n",
    "        \n",
    "      name_1 = 'train'\n",
    "      #name_2 = 'vali'\n",
    "      training_generator = DataGenerator(listID_train, name_1 , **params)\n",
    "      #validation_generator = DataGenerator(listID_vali, name_2, **params)\n",
    "      \n",
    "      spe = int(np.floor(len(listID_train) / batch_size))\n",
    "        \n",
    "      #print('Steps per epoch: ',spe)\n",
    "      #vs = int(np.floor(len(listID_vali) / batch_size))\n",
    "      #print(\"Validation Steps: \",vs)\n",
    "    \n",
    "      model = check_print()\n",
    "      #model.summary()\n",
    "      \n",
    "      print(\"Testset Before Training \")\n",
    "      score = model.evaluate(x_test,y_test)\n",
    "        \n",
    "      print(\"Loss :\",score[0])\n",
    "      print(\"Accuracy :\",score[1])\n",
    "        \n",
    "      results = []\n",
    "      start_time = time.time()\n",
    "      \n",
    "      history = model.fit_generator(generator=training_generator,\n",
    "                          steps_per_epoch = spe,\n",
    "                          validation_data=(x_vali,y_vali),\n",
    "                          use_multiprocessing=True,\n",
    "                          epochs=EPOCH, \n",
    "                          verbose=1,\n",
    "                          workers = 30,\n",
    "                          max_queue_size=40)\n",
    "\n",
    "            \n",
    "      average_time_per_epoch = (time.time() - start_time) / EPOCH\n",
    "      results.append((history, average_time_per_epoch))\n",
    "      Plot(results)\n",
    "     \n",
    "      print(\"Testset After Training \")\n",
    "      score = model.evaluate(x_test,y_test)\n",
    "      print(\"Loss :\",score[0])\n",
    "      print(\"Accuracy :\",score[1])\n",
    "      \n",
    "        \n",
    "#     from math import ceil\n",
    "#     path_tfrecords_train = '/train/execute/AffectNet/Data/affectNet.tfrecords'\n",
    "    \n",
    "#     model = check_print()\n",
    "#     model.summary()\n",
    "#     model.fit_generator(generate_batch_data_random(410176,path_tfrecords_train),epochs=EPOCH,validation_data=(x_vali, y_vali),steps_per_epoch = 25636)  \n",
    "    \n",
    "    \n",
    "#     #model_keras.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc',top_k_categorical_accuracy])\n",
    "#     est_ = tf.keras.estimator.model_to_estimator(keras_model=model_keras)\n",
    "#     train_spec = tf.estimator.TrainSpec(input_fn=lambda: imgs_input_fn(path_tfrecords_train,perform_shuffle=True,repeat_count=5,batch_size=32), max_steps=500)\n",
    "#     eval_spec = tf.estimator.EvalSpec(input_fn=lambda: imgs_input_fn(path_tfrecords_vali, perform_shuffle=False, batch_size=1))\n",
    "#     #est_(input_fn=train_spec, steps=2000)\n",
    "#     est_.train_and_evaluate(train_spec=train_spec, eval_spec=eval_spec)\n",
    "    \n",
    "    \n",
    "    #train_spec = tf.estimator.TrainSpec(input_fn=lambda: imgs_input_fn(path_tfrecords_train,perform_shuffle=True,repeat_count=5,batch_size=32), max_steps=500)\n",
    "    #eval_spec = tf.estimator.EvalSpec(input_fn=lambda: imgs_input_fn(path_tfrecords_vali, perform_shuffle=False, batch_size=1))\n",
    "    #tf.estimator.train_and_evaluate(estimator=est_model, train_spec=train_spec, eval_spec=eval_spec)\n",
    "    \n",
    "#     images, labels = imgs_input_fn(path_tfrecords_train)\n",
    "#     print(type(images))\n",
    "#     #model_input = keras.Input(tensor=images, shape=(224, 224,3), dtype=tf.uint8, name=\"input\")\n",
    "#     model_input = Input(shape=(224, 224, 3))\n",
    "#     model_output = model(tensor = images,shape = model_input)\n",
    "#     model = keras.models.Model(inputs=model_input, outputs=model_output)\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc',top_k_categorical_accuracy],target_tensor = [labels])\n",
    "#     model.fit(steps_per_epoch=5000, epochs=EPOCH)\n",
    "\n",
    "#     print(x_train.shape)\n",
    "#     print(y_train.shape)\n",
    "#     print(x_test.shape)\n",
    "#     print(y_test.shape)\n",
    "#     print(x_vali.shape)\n",
    "#     print(y_vali.shape)\n",
    "#     y_train = to_categorical(y_train, NB_CLASS)\n",
    "#     y_test = to_categorical(y_test, NB_CLASS)\n",
    "#     y_vali = to_categorical(y_vali, NB_CLASS)\n",
    "\n",
    "    #print(inpt.shape)\n",
    "\n",
    "    #if os.path.exists('resnet_34.h5'):\n",
    "    #    model=load_model('resnet_34.h5')\n",
    "    #else:\n",
    "    #model=check_print()\n",
    "\n",
    "#     results = []\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     history = model.fit(x_train, y_train,\n",
    "#               batch_size = batch_size,\n",
    "#               epochs= EPOCH,\n",
    "#               validation_data=(x_vali,y_vali))\n",
    "\n",
    "#     #scores = model.evaluate(x_test,to_categorical(y_test))\n",
    "#     #print(\"Accuracy: \",scores[1])\n",
    "#     #print(\"Scores[0] :\",scores[0])\n",
    "\n",
    "#     average_time_per_epoch = (time.time() - start_time) / EPOCH\n",
    "#     results.append((history, average_time_per_epoch))\n",
    "#     plt.style.use('ggplot')\n",
    "#     ax1 = plt.subplot2grid((2, 2), (0, 0))\n",
    "#     ax1.set_title('Accuracy')\n",
    "#     ax1.set_ylabel('Accuracy')\n",
    "#     ax1.set_xlabel('Epochs')\n",
    "#     ax2 = plt.subplot2grid((2, 2), (0, 1))\n",
    "#     ax2.set_title('Loss')\n",
    "#     ax2.set_ylabel('Loss')\n",
    "#     ax2.set_xlabel('Epochs')\n",
    "# #     ax3 = plt.subplot2grid((2, 2), (0, 1), rowspan=2)\n",
    "# #     ax3.set_title('Time')\n",
    "# #     ax3.set_ylabel('Seconds')\n",
    "\n",
    "#     for result in results:\n",
    "#         ax1.plot(result[0].epoch, result[0].history['val_acc'], label='Vali')\n",
    "#         ax1.plot(result[0].epoch, result[0].history['acc'], label='Train')\n",
    "#         ax2.plot(result[0].epoch, result[0].history['val_loss'], label='Vali')\n",
    "#         ax2.plot(result[0].epoch, result[0].history['loss'], label='Train')\n",
    "\n",
    "#     ax1.legend()\n",
    "#     ax2.legend()\n",
    "#     #ax3.bar(np.arange(len(results)), [x[1] for x in results],\n",
    "#     #        align='center')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     score = model.evaluate(x_test, y_test, verbose=0)\n",
    "#     print('Test loss:', score[0])\n",
    "#     print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
