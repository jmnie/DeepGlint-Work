{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## First Part: Load the data: \n",
    "# import pandas as pd \n",
    "# import numpy as np\n",
    "# import scipy.io as sio\n",
    "\n",
    "# def shuffle(x_,y_):\n",
    "#     s = np.arange(x_.shape[0])\n",
    "#     s = np.random.shuffle(s)\n",
    "\n",
    "#     x_re = x_[s]\n",
    "#     y_re = y_[s]\n",
    "\n",
    "#     x_re = np.reshape(x_re,(len(x_),48,48))\n",
    "#     y_re = np.reshape(y_re,(len(y_)))\n",
    "#     return x_re,y_re\n",
    "\n",
    "# def read_fer(path):\n",
    "#     # train_path = \"C:\\\\Users\\\\Jiaming Nie\\\\Documents\\\\Work-DeepGlint\\Facial\\datasets\\\\train.csv\"\n",
    "#     data = pd.read_csv(path, dtype='a')\n",
    "#     label = np.array(data['emotion'])\n",
    "#     img_data = np.array(data['pixels'])\n",
    "\n",
    "#     N_sample = label.size\n",
    "\n",
    "#     x_data = np.zeros((N_sample, 48 * 48))\n",
    "#     # train_label = np.zeros((N_sample, 7), dtype=int)\n",
    "#     y_label = np.zeros(N_sample, dtype=int)\n",
    "#     # print(train_label)\n",
    "\n",
    "#     for i in range(N_sample):\n",
    "#         x = img_data[i]\n",
    "#         x = np.fromstring(x, dtype=float, sep=' ')\n",
    "#         x_max = x.max()\n",
    "#         x = x / (x_max + 0.0001)\n",
    "#         # print x_max\n",
    "#         # print x\n",
    "#         x_data[i] = x\n",
    "#         y_label[i] = int(label[i])\n",
    "#         # train_label[i, label[i]] = 1 #This step seems direct one-hot encoding\n",
    "#         # print(y_label[i])\n",
    "#         #    img_x = np.reshape(x, (48, 48))\n",
    "#         #    plt.subplot(10,10,i+1)\n",
    "#         #    plt.axis('off')\n",
    "#         #    plt.imshow(img_x, plt.cm.gray)\n",
    "\n",
    "#     x_data = np.reshape(x_data,(len(x_data),48,48))\n",
    "#     return x_data, y_label\n",
    "\n",
    "# def ReadData_fer():\n",
    "#     # ubuntu path\n",
    "#     #path_train = \"/home/jiaming/code/DeepGlint-Work/Facial/datasets/train.csv\"\n",
    "#     #path_test = \"/home/jiaming/code/DeepGlint-Work/Facial/datasets/test.csv\"\n",
    "\n",
    "#     # windows path\n",
    "#     path_train = \"/train/trainset/1/train.csv\"\n",
    "#     path_test = \"/train/trainset/1/test.csv\"\n",
    "#     path_vali = \"/train/trainset/1/val.csv\"\n",
    "    \n",
    "#     x_train, y_train = read_fer(path_train)\n",
    "#     x_test, y_test = read_fer(path_test)\n",
    "#     x_vali, y_vali = read_fer(path_vali)\n",
    "\n",
    "#     x_train, y_train = shuffle(x_train, y_train)\n",
    "#     x_test, y_test = shuffle(x_test, y_test)\n",
    "#     x_vali, y_vali = shuffle(x_vali, y_vali)\n",
    "\n",
    "#     return x_train,y_train,x_test,y_test,x_vali,y_vali\n",
    "\n",
    "# def zca_whitening(X):\n",
    "#     \"\"\"\n",
    "#         Function to compute ZCA whitening matrix (aka Mahalanobis whitening).\n",
    "#         INPUT:  X: [M x N] matrix.\n",
    "#             Rows: Variables\n",
    "#             Columns: Observations\n",
    "#         OUTPUT: ZCAMatrix: [M x M] matrix\n",
    "#         \"\"\"\n",
    "#     mean_ = np.mean(X)\n",
    "#     X = X - mean_\n",
    "#     # Covariance matrix [column-wise variables]: Sigma = (X-mu)' * (X-mu) / N\n",
    "#     sigma = np.cov(X, rowvar=True)  # [M x M]\n",
    "#     # Singular Value Decomposition. X = U * np.diag(S) * V\n",
    "#     U, S, V = np.linalg.svd(sigma)\n",
    "#     # U: [M x M] eigenvectors of sigma.\n",
    "#     # S: [M x 1] eigenvalues of sigma.\n",
    "#     # V: [M x M] transpose of U\n",
    "#     # Whitening constant: prevents division by zero\n",
    "#     epsilon = 0.1\n",
    "#     # ZCA Whitening matrix: U * Lambda * U'\n",
    "#     ZCAMatrix = np.dot(U, np.dot(np.diag(1.0 / np.sqrt(S + epsilon)), U.T))  # [M x M]\n",
    "#     return ZCAMatrix\n",
    "\n",
    "# def normalization(x_):\n",
    "\n",
    "#     length = len(x_)\n",
    "\n",
    "#     for i in range(length):\n",
    "#         x_[i] = zca_whitening(x_[i])\n",
    "\n",
    "#     return x_\n",
    "\n",
    "# def oneHot(y_):\n",
    "#     # Function to encode output labels from number indexes \n",
    "#     # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "#     y_ = y_.reshape(len(y_))\n",
    "#     n_values = int(np.max(y_)) + 1\n",
    "#     return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,y_train,x_test,y_test,x_vali,y_vali = ReadData_fer()\n",
    "\n",
    "# # Normalization\n",
    "# #x_train = normalization(x_train)\n",
    "# #x_test = normalization(x_test)\n",
    "# #x_vali = normalization(x_vali)\n",
    "\n",
    "# x_train = x_train.reshape((len(x_train), 48, 48, 1))\n",
    "# x_test = x_test.reshape((len(x_test), 48, 48, 1))\n",
    "# x_vali = x_vali.reshape((len(x_vali),48,48,1))\n",
    "\n",
    "# train_name = 'x_train.mat'\n",
    "# vali_name = 'x_vali.mat'\n",
    "# test_name = 'x_test.mat'\n",
    "# sio.savemat(train_name,{'x_train': x_train})\n",
    "# sio.savemat(test_name,{'x_test': x_test})\n",
    "# sio.savemat(vali_name,{'x_vali': x_vali})\n",
    "# np.savetxt('y_train.txt',y_train)\n",
    "# np.savetxt('y_test.txt',y_test)\n",
    "# np.savetxt('y_vali.txt',y_vali)\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "\n",
    "def normalization(x_):\n",
    "    length = len(x_)\n",
    "    max_ = np.amax(x_)\n",
    "    min_ = np.min(x_)\n",
    "    x_ = 2*(x_ - min_)/(max_ - min_)\n",
    "    return x_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, concatenate, \\\n",
    "    Activation, ZeroPadding2D\n",
    "from keras.layers import add, Flatten\n",
    "#from keras.utils import plot_model\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import scipy.io\n",
    "from keras.regularizers import * \n",
    "from keras.optimizers import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import time\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "KTF.set_session(tf.Session(config=tf.ConfigProto(device_count={'gpu':0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "EPOCH = 100\n",
    "NB_CLASS=7\n",
    "IM_WIDTH=48\n",
    "IM_HEIGHT=48\n",
    "CHANNEL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2d_BN(x, nb_filter, kernel_size, strides=(1, 1), padding='same', name=None):\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "\n",
    "    #x = Conv2D(nb_filter, kernel_size, padding=padding, strides=strides, activation='relu', name=conv_name)(x)\n",
    "    #x = BatchNormalization(axis=3, name=bn_name)(x)\n",
    "    x = Conv2D(nb_filter, kernel_size, padding=padding, strides=strides,name=conv_name)(x)\n",
    "    x = BatchNormalization(axis=3, name=bn_name)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def identity_Block(inpt, nb_filter, kernel_size, strides=(1, 1), with_conv_shortcut=False):\n",
    "    x = Conv2d_BN(inpt, nb_filter=nb_filter, kernel_size=kernel_size, strides=strides, padding='same')\n",
    "    x = Conv2d_BN(x, nb_filter=nb_filter, kernel_size=kernel_size, padding='same')\n",
    "    if with_conv_shortcut:\n",
    "        shortcut = Conv2d_BN(inpt, nb_filter=nb_filter, strides=strides, kernel_size=kernel_size)\n",
    "        x = add([x, shortcut])\n",
    "        return x\n",
    "    else:\n",
    "        x = add([x, inpt])\n",
    "        return x\n",
    "\n",
    "def bottleneck_Block(inpt,nb_filters,strides=(1,1),with_conv_shortcut=False):\n",
    "    k1,k2,k3=nb_filters\n",
    "    x = Conv2d_BN(inpt, nb_filter=k1, kernel_size=1, strides=strides, padding='same')\n",
    "    x = Conv2d_BN(x, nb_filter=k2, kernel_size=3, padding='same')\n",
    "    x = Conv2d_BN(x, nb_filter=k3, kernel_size=1, padding='same')\n",
    "    if with_conv_shortcut:\n",
    "        shortcut = Conv2d_BN(inpt, nb_filter=k3, strides=strides, kernel_size=1)\n",
    "        x = add([x, shortcut])\n",
    "        return x\n",
    "    else:\n",
    "        x = add([x, inpt])\n",
    "        return x\n",
    "\n",
    "def resnet_34(width,height,channel,classes):\n",
    "    inpt = Input(shape=(width, height, channel))\n",
    "    x = ZeroPadding2D((3, 3))(inpt)\n",
    "\n",
    "    #conv1\n",
    "    x = Conv2d_BN(x, nb_filter=64, kernel_size=(7, 7), strides=(2, 2), padding='valid')\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = Dropout(0.5)(x)\n",
    "    #conv2_x\n",
    "    x = identity_Block(x, nb_filter=64, kernel_size=(3, 3))\n",
    "    #x = identity_Block(x, nb_filter=64, kernel_size=(3, 3))\n",
    "    x = identity_Block(x, nb_filter=64, kernel_size=(3, 3))\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    #conv3_x\n",
    "    #x = identity_Block(x, nb_filter=128, kernel_size=(3, 3), strides=(2, 2), with_conv_shortcut=True)\n",
    "    #x = identity_Block(x, nb_filter=128, kernel_size=(3, 3))\n",
    "    #x = identity_Block(x, nb_filter=128, kernel_size=(3, 3))\n",
    "    #x = identity_Block(x, nb_filter=128, kernel_size=(3, 3))\n",
    "    x = AveragePooling2D(pool_size=(6,6))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #conv4_x\n",
    "    #x = identity_Block(x, nb_filter=256, kernel_size=(3, 3), strides=(2, 2), with_conv_shortcut=True)\n",
    "    #x = identity_Block(x, nb_filter=256, kernel_size=(3, 3))\n",
    "    #x = identity_Block(x, nb_filter=256, kernel_size=(3, 3))\n",
    "    #x = identity_Block(x, nb_filter=256, kernel_size=(3, 3))\n",
    "    #x = identity_Block(x, nb_filter=256, kernel_size=(3, 3))\n",
    "    #x = identity_Block(x, nb_filter=256, kernel_size=(3, 3))\n",
    "\n",
    "    #conv5_x\n",
    "    #x = identity_Block(x, nb_filter=512, kernel_size=(3, 3), strides=(2, 2), with_conv_shortcut=True)\n",
    "    #x = identity_Block(x, nb_filter=512, kernel_size=(3, 3))\n",
    "    #x = identity_Block(x, nb_filter=512, kernel_size=(3, 3))\n",
    "    #x = AveragePooling2D(pool_size=(7, 7))(x)\n",
    "    #x = AveragePooling2D(pool_size=(2,2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes, activation='softmax',W_regularizer=l2(0.001))(x)\n",
    "\n",
    "    model = Model(inputs=inpt, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def acc_top2(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "\n",
    "def check_print():\n",
    "    # Create a Keras Model\n",
    "    model = resnet_34(IM_WIDTH,IM_HEIGHT,1,NB_CLASS)\n",
    "    #model.summary()\n",
    "    # Save a PNG of the Model Build\n",
    "    #plot_model(model, to_file='resnet.png')\n",
    "    adam = Adam(lr=0.1)\n",
    "    #sgd = SGD(lr=0.1, momentum=0.0, decay=0.001, nesterov=True)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc',top_k_categorical_accuracy])\n",
    "    print('Model Compiled')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Data Successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:78: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(7, activation=\"softmax\", kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compiled\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 48, 48, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D) (None, 54, 54, 1)     0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 24, 24, 64)    3200        zero_padding2d_1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 24, 24, 64)    256         conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 24, 24, 64)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 12, 12, 64)    0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 12, 12, 64)    0           max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 12, 12, 64)    36928       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 12, 12, 64)    256         conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 12, 12, 64)    0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 12, 12, 64)    36928       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 12, 12, 64)    256         conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 12, 12, 64)    0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 12, 12, 64)    0           activation_3[0][0]               \n",
      "                                                                   dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 12, 12, 64)    36928       add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 12, 12, 64)    256         conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 12, 12, 64)    0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 12, 12, 64)    36928       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 12, 12, 64)    256         conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 12, 12, 64)    0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 12, 12, 64)    0           activation_5[0][0]               \n",
      "                                                                   add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 12, 12, 64)    0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePool (None, 2, 2, 64)      0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 2, 2, 64)      0           average_pooling2d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0           dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 7)             1799        flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 153,991\n",
      "Trainable params: 153,351\n",
      "Non-trainable params: 640\n",
      "____________________________________________________________________________________________________\n",
      "Not Training\n",
      "('Test loss:', 2.0787283036065136)\n",
      "('Test accuracy:', 0.13908530426102625)\n",
      "Epoch 1/100\n",
      "14452/28709 [==============>...............] - ETA: 268s - loss: 1.8381 - acc: 0.2466 - top_k_categorical_accuracy: 0.8666"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-86525f357260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vali\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_vali\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                        )\n\u001b[1;32m     64\u001b[0m     \u001b[0maverage_time_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2042\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #x_train, y_train, x_test, y_test, x_vali, y_vali = ReadData_fer()\n",
    "    \n",
    "    x_train = scipy.io.loadmat('/train/execute/fer2013/x_train.mat')['x_train']\n",
    "    x_test = scipy.io.loadmat('/train/execute/fer2013/x_test.mat')['x_test']\n",
    "    x_vali = scipy.io.loadmat('/train/execute/fer2013/x_vali.mat')['x_vali']\n",
    "    y_train = np.loadtxt('/train/execute/fer2013/y_train.txt')\n",
    "    y_test = np.loadtxt('/train/execute/fer2013/y_test.txt')\n",
    "    y_vali = np.loadtxt('/train/execute/fer2013/y_vali.txt')\n",
    "    \n",
    "    # Normalization\n",
    "    x_train = normalization(x_train)\n",
    "    x_test = normalization(x_test)\n",
    "    x_vali = normalization(x_vali)\n",
    "\n",
    "    x_train = x_train.reshape((len(x_train), 48, 48, 1))\n",
    "    x_test = x_test.reshape((len(x_test), 48, 48, 1))\n",
    "    x_vali = x_vali.reshape((len(x_vali),48,48,1))\n",
    "    \n",
    "    y_train = to_categorical(y_train, NB_CLASS)\n",
    "    y_test = to_categorical(y_test, NB_CLASS)\n",
    "    y_vali = to_categorical(y_vali, NB_CLASS)\n",
    "    \n",
    "    \n",
    "    ## Data Augmentation \n",
    "    train_datagen = ImageDataGenerator(\n",
    "        #zca_whitening = True,\n",
    "        shear_range = 0.2,\n",
    "        width_shift_range = 0.05,\n",
    "        horizontal_flip=True)\n",
    "    \n",
    "    vali_datagen = ImageDataGenerator(\n",
    "        #zca_whitening = True,\n",
    "        shear_range = 0.2,\n",
    "        width_shift_range = 0.05,\n",
    "        horizontal_flip=True)\n",
    "        \n",
    "    train_datagen.fit(x_train)\n",
    "    \n",
    "    print(\"Read Data Successful\")\n",
    "    #print(x_train.shape)\n",
    "    #print(y_train.shape)\n",
    "    #print(x_test.shape)\n",
    "    #print(y_test.shape)\n",
    "    #print(x_vali.shape)\n",
    "    #print(y_vali.shape)\n",
    "\n",
    "    model = check_print()\n",
    "    model.summary()\n",
    "    \n",
    "    print(\"Not Training\")\n",
    "    score_0 = model.evaluate(x_train, y_train, verbose=0)\n",
    "    print('Test loss:', score_0[0])\n",
    "    print('Test accuracy:', score_0[1])\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    history = model.fit_generator(train_datagen.flow(x_train,y_train,batch_size = batch_size),\n",
    "                        steps_per_epoch = x_train.shape[0],\n",
    "                        epochs = EPOCH,\n",
    "                        validation_data=(x_vali,y_vali),\n",
    "                       )\n",
    "    average_time_per_epoch = (time.time() - start_time) / EPOCH\n",
    "    results.append((history, average_time_per_epoch))\n",
    "    plt.style.use('ggplot')\n",
    "    ax1 = plt.subplot2grid((2, 2), (0, 0))\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax2 = plt.subplot2grid((2, 2), (1, 0))\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax3 = plt.subplot2grid((2, 2), (0, 1), rowspan=2)\n",
    "    ax3.set_title('Time')\n",
    "    ax3.set_ylabel('Seconds')\n",
    "    \n",
    "    for result in results:\n",
    "        ax1.plot(result[0].epoch, result[0].history['val_acc'], label='Vali')\n",
    "        ax1.plot(result[0].epoch, result[0].history['acc'], label='Train')\n",
    "        ax2.plot(result[0].epoch, result[0].history['val_loss'], label='Vali')\n",
    "        ax2.plot(result[0].epoch, result[0].history['loss'], label='Train')\n",
    "\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    ax3.bar(np.arange(len(results)), [x[1] for x in results],\n",
    "            align='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    score = model.evaluate(x_test, to_categorical(y_test), verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "\n",
    "#     #x_train, y_train, x_test, y_test, x_vali, y_vali = ReadData_fer()\n",
    "    \n",
    "#     x_train = scipy.io.loadmat('/train/execute/fer2013/x_train.mat')['x_train']\n",
    "#     x_test = scipy.io.loadmat('/train/execute/fer2013/x_test.mat')['x_test']\n",
    "#     x_vali = scipy.io.loadmat('/train/execute/fer2013/x_vali.mat')['x_vali']\n",
    "#     y_train = np.loadtxt('/train/execute/fer2013/y_train.txt')\n",
    "#     y_test = np.loadtxt('/train/execute/fer2013/y_test.txt')\n",
    "#     y_vali = np.loadtxt('/train/execute/fer2013/y_vali.txt')\n",
    "#     # Normalization\n",
    "#     #x_train = normalization(x_train)\n",
    "#     #x_test = normalization(x_test)\n",
    "#     #x_vali = normalization(x_vali)\n",
    "\n",
    "#     x_train = x_train.reshape((len(x_train), 48, 48, 1))\n",
    "#     x_test = x_test.reshape((len(x_test), 48, 48, 1))\n",
    "#     x_vali = x_vali.reshape((len(x_vali),48,48,1))\n",
    "    \n",
    "#     x_train /= 255\n",
    "#     x_test /= 255\n",
    "#     x_vali /= 255\n",
    "#     print(x_train.shape)\n",
    "#     print(x_test.shape)\n",
    "\n",
    "#     model = check_print()\n",
    "#     model.summary()\n",
    "    \n",
    "#     score = model.evaluate(x_train,to_categorical(y_train),verbose=0)\n",
    "#     print(\"Non - Training\")\n",
    "#     print('Train loss:', score[0])\n",
    "#     print('Train accuracy:', score[1])\n",
    "    \n",
    "#     results = []\n",
    "#     start_time = time.time()\n",
    "#     history = model.fit(x_train,to_categorical(y_train),\n",
    "#               epochs=EPOCH,\n",
    "#               verbose=1,\n",
    "#               validation_data=(x_vali, to_categorical(y_vali))\n",
    "#     )\n",
    "# #     average_time_per_epoch = (time.time() - start_time) / EPOCH\n",
    "# #     results.append((history, average_time_per_epoch))\n",
    "# #     plt.style.use('ggplot')\n",
    "# #     ax1 = plt.subplot2grid((2, 2), (0, 0))\n",
    "# #     ax1.set_title('Accuracy')\n",
    "# #     ax1.set_ylabel('Accuracy')\n",
    "# #     ax1.set_xlabel('Epochs')\n",
    "# #     ax2 = plt.subplot2grid((2, 2), (1, 0))\n",
    "# #     ax2.set_title('Loss')\n",
    "# #     ax2.set_ylabel('Loss')\n",
    "# #     ax2.set_xlabel('Epochs')\n",
    "# #     ax3 = plt.subplot2grid((2, 2), (0, 1), rowspan=2)\n",
    "# #     ax3.set_title('Time')\n",
    "# #     ax3.set_ylabel('Seconds')\n",
    "    \n",
    "# #     for result in results:\n",
    "# #         ax1.plot(result[0].epoch, result[0].history['val_acc'], label='Vali')\n",
    "# #         ax1.plot(result[0].epoch, result[0].history['acc'], label='Train')\n",
    "# #         ax2.plot(result[0].epoch, result[0].history['val_loss'], label='Vali')\n",
    "# #         ax2.plot(result[0].epoch, result[0].history['loss'], label='Train')\n",
    "\n",
    "# #     ax1.legend()\n",
    "# #     ax2.legend()\n",
    "# #     ax3.bar(np.arange(len(results)), [x[1] for x in results],\n",
    "# #             align='center')\n",
    "# #     plt.tight_layout()\n",
    "# #     plt.show()\n",
    "    \n",
    "# #     score = model.evaluate(x_test, to_categorical(y_test), verbose=0)\n",
    "# #     print('Test loss:', score[0])\n",
    "# #     print('Test accuracy:', score[1])\n",
    "    \n",
    "#     # model.fit_generator(train_generator,validation_data=vaild_generator,epochs=EPOCH,steps_per_epoch=train_generator.n/batch_size\n",
    "#     #                     ,validation_steps=vaild_generator.n/batch_size)\n",
    "#     # model.save('resnet_50.h5')\n",
    "#     # loss,acc,top_acc=model.evaluate_generator(test_generator, steps=test_generator.n / batch_size)\n",
    "#     # print('Test result:loss:%f,acc:%f,top_acc:%f' % (loss, acc, top_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
