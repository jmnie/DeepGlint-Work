{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First Part: Load the data: \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "def shuffle(x_,y_):\n",
    "    s = np.arange(x_.shape[0])\n",
    "    s = np.random.shuffle(s)\n",
    "\n",
    "    x_re = x_[s]\n",
    "    y_re = y_[s]\n",
    "\n",
    "    x_re = np.reshape(x_re,(len(x_),48,48))\n",
    "    y_re = np.reshape(y_re,(len(y_)))\n",
    "    return x_re,y_re\n",
    "\n",
    "def read_fer(path):\n",
    "    # train_path = \"C:\\\\Users\\\\Jiaming Nie\\\\Documents\\\\Work-DeepGlint\\Facial\\datasets\\\\train.csv\"\n",
    "    data = pd.read_csv(path, dtype='a')\n",
    "    label = np.array(data['emotion'])\n",
    "    img_data = np.array(data['pixels'])\n",
    "\n",
    "    N_sample = label.size\n",
    "\n",
    "    x_data = np.zeros((N_sample, 48 * 48))\n",
    "    # train_label = np.zeros((N_sample, 7), dtype=int)\n",
    "    y_label = np.zeros(N_sample, dtype=int)\n",
    "    # print(train_label)\n",
    "\n",
    "    for i in range(N_sample):\n",
    "        x = img_data[i]\n",
    "        x = np.fromstring(x, dtype=float, sep=' ')\n",
    "        x_max = x.max()\n",
    "        x = x / (x_max + 0.0001)\n",
    "        # print x_max\n",
    "        # print x\n",
    "        x_data[i] = x\n",
    "        y_label[i] = int(label[i])\n",
    "        # train_label[i, label[i]] = 1 #This step seems direct one-hot encoding\n",
    "        # print(y_label[i])\n",
    "        #    img_x = np.reshape(x, (48, 48))\n",
    "        #    plt.subplot(10,10,i+1)\n",
    "        #    plt.axis('off')\n",
    "        #    plt.imshow(img_x, plt.cm.gray)\n",
    "\n",
    "    x_data = np.reshape(x_data,(len(x_data),48,48))\n",
    "    return x_data, y_label\n",
    "\n",
    "def ReadData_fer():\n",
    "    # ubuntu path\n",
    "    #path_train = \"/home/jiaming/code/DeepGlint-Work/Facial/datasets/train.csv\"\n",
    "    #path_test = \"/home/jiaming/code/DeepGlint-Work/Facial/datasets/test.csv\"\n",
    "\n",
    "    # windows path\n",
    "    path_train = \"/train/trainset/1/train.csv\"\n",
    "    path_test = \"/train/trainset/1/test.csv\"\n",
    "    path_vali = \"/train/trainset/1/val.csv\"\n",
    "    \n",
    "    x_train, y_train = read_fer(path_train)\n",
    "    x_test, y_test = read_fer(path_test)\n",
    "    x_vali, y_vali = read_fer(path_vali)\n",
    "\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    x_test, y_test = shuffle(x_test, y_test)\n",
    "    x_vali, y_vali = shuffle(x_vali, y_vali)\n",
    "\n",
    "    return x_train,y_train,x_test,y_test,x_vali,y_vali\n",
    "\n",
    "def zca_whitening(X):\n",
    "    \"\"\"\n",
    "        Function to compute ZCA whitening matrix (aka Mahalanobis whitening).\n",
    "        INPUT:  X: [M x N] matrix.\n",
    "            Rows: Variables\n",
    "            Columns: Observations\n",
    "        OUTPUT: ZCAMatrix: [M x M] matrix\n",
    "        \"\"\"\n",
    "    mean_ = np.mean(X)\n",
    "    X = X - mean_\n",
    "    # Covariance matrix [column-wise variables]: Sigma = (X-mu)' * (X-mu) / N\n",
    "    sigma = np.cov(X, rowvar=True)  # [M x M]\n",
    "    # Singular Value Decomposition. X = U * np.diag(S) * V\n",
    "    U, S, V = np.linalg.svd(sigma)\n",
    "    # U: [M x M] eigenvectors of sigma.\n",
    "    # S: [M x 1] eigenvalues of sigma.\n",
    "    # V: [M x M] transpose of U\n",
    "    # Whitening constant: prevents division by zero\n",
    "    epsilon = 0.1\n",
    "    # ZCA Whitening matrix: U * Lambda * U'\n",
    "    ZCAMatrix = np.dot(U, np.dot(np.diag(1.0 / np.sqrt(S + epsilon)), U.T))  # [M x M]\n",
    "    return ZCAMatrix\n",
    "\n",
    "def normalization(x_):\n",
    "\n",
    "    length = len(x_)\n",
    "\n",
    "    for i in range(length):\n",
    "        x_[i] = zca_whitening(x_[i])\n",
    "\n",
    "    return x_\n",
    "\n",
    "def oneHot(y_):\n",
    "    # Function to encode output labels from number indexes \n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 48, 48, 1)\n",
      "(3589, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test,x_vali,y_vali = ReadData_fer()\n",
    "\n",
    "# Normalization\n",
    "# x_train = normalization(x_train)\n",
    "# x_test = normalization(x_test)\n",
    "# x_vali = normalization(x_vali)\n",
    "\n",
    "x_train = x_train.reshape((len(x_train), 48, 48, 1))\n",
    "x_test = x_test.reshape((len(x_test), 48, 48, 1))\n",
    "x_vali = x_vali.reshape((len(x_vali),48,48,1))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title='Normalized confusion matrix'\n",
    "    else:\n",
    "        title='Confusion matrix'\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "## multiclass or binary report\n",
    "## If binary (sigmoid output), set binary parameter to True\n",
    "def full_multiclass_report(model,\n",
    "                           x,\n",
    "                           y_true,\n",
    "                           classes,\n",
    "                           batch_size=32,\n",
    "                           binary=False):\n",
    "\n",
    "    # 1. Transform one-hot encoded y_true into their class number\n",
    "    if not binary:\n",
    "        y_true = np.argmax(y_true,axis=1)\n",
    "    \n",
    "    # 2. Predict classes and stores in y_pred\n",
    "    y_pred = model.predict_classes(x, batch_size=batch_size)\n",
    "    \n",
    "    # 3. Print accuracy score\n",
    "    print(\"Accuracy : \"+ str(accuracy_score(y_true,y_pred)))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    # 4. Print classification report\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_true,y_pred,digits=5))    \n",
    "    \n",
    "    # 5. Plot confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_true,y_pred)\n",
    "    print(cnf_matrix)\n",
    "    plot_confusion_matrix(cnf_matrix,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_160 (Conv2D)          (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_161 (Conv2D)          (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_162 (Conv2D)          (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_70 (MaxPooling (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_163 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_164 (Conv2D)          (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_165 (Conv2D)          (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_71 (MaxPooling (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_166 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_167 (Conv2D)          (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_168 (Conv2D)          (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_72 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 779,783\n",
      "Trainable params: 779,783\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Non-Training\n",
      "('Train loss:', 1.9533903143067002)\n",
      "('Train accuracy:', 0.21756243686648785)\n",
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/40\n",
      "28709/28709 [==============================] - 292s 10ms/step - loss: 1.8316 - acc: 0.2488 - val_loss: 1.8252 - val_acc: 0.2494\n",
      "Epoch 2/40\n",
      "28709/28709 [==============================] - 308s 11ms/step - loss: 1.8179 - acc: 0.2513 - val_loss: 1.8247 - val_acc: 0.2494\n",
      "Epoch 3/40\n",
      "28709/28709 [==============================] - 362s 13ms/step - loss: 1.8176 - acc: 0.2513 - val_loss: 1.8244 - val_acc: 0.2494\n",
      "Epoch 4/40\n",
      "28709/28709 [==============================] - 355s 12ms/step - loss: 1.8173 - acc: 0.2513 - val_loss: 1.8244 - val_acc: 0.2494\n",
      "Epoch 5/40\n",
      "28709/28709 [==============================] - 338s 12ms/step - loss: 1.8175 - acc: 0.2513 - val_loss: 1.8243 - val_acc: 0.2494\n",
      "Epoch 6/40\n",
      "28709/28709 [==============================] - 349s 12ms/step - loss: 1.8172 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 7/40\n",
      "28709/28709 [==============================] - 349s 12ms/step - loss: 1.8171 - acc: 0.2513 - val_loss: 1.8241 - val_acc: 0.2494\n",
      "Epoch 8/40\n",
      "28709/28709 [==============================] - 357s 12ms/step - loss: 1.8173 - acc: 0.2513 - val_loss: 1.8241 - val_acc: 0.2494\n",
      "Epoch 9/40\n",
      "28709/28709 [==============================] - 373s 13ms/step - loss: 1.8169 - acc: 0.2513 - val_loss: 1.8243 - val_acc: 0.2494\n",
      "Epoch 10/40\n",
      "28709/28709 [==============================] - 371s 13ms/step - loss: 1.8173 - acc: 0.2513 - val_loss: 1.8243 - val_acc: 0.2494\n",
      "Epoch 11/40\n",
      "28709/28709 [==============================] - 347s 12ms/step - loss: 1.8172 - acc: 0.2513 - val_loss: 1.8243 - val_acc: 0.2494\n",
      "Epoch 12/40\n",
      "28709/28709 [==============================] - 349s 12ms/step - loss: 1.8168 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 13/40\n",
      "28709/28709 [==============================] - 375s 13ms/step - loss: 1.8174 - acc: 0.2513 - val_loss: 1.8241 - val_acc: 0.2494\n",
      "Epoch 14/40\n",
      "28709/28709 [==============================] - 374s 13ms/step - loss: 1.8172 - acc: 0.2513 - val_loss: 1.8241 - val_acc: 0.2494\n",
      "Epoch 15/40\n",
      "28709/28709 [==============================] - 375s 13ms/step - loss: 1.8170 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 16/40\n",
      "28709/28709 [==============================] - 384s 13ms/step - loss: 1.8169 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 17/40\n",
      "28709/28709 [==============================] - 382s 13ms/step - loss: 1.8169 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 18/40\n",
      "28709/28709 [==============================] - 379s 13ms/step - loss: 1.8171 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 19/40\n",
      "28709/28709 [==============================] - 380s 13ms/step - loss: 1.8168 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 20/40\n",
      "28709/28709 [==============================] - 378s 13ms/step - loss: 1.8169 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 21/40\n",
      "28709/28709 [==============================] - 384s 13ms/step - loss: 1.8170 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 22/40\n",
      "28709/28709 [==============================] - 382s 13ms/step - loss: 1.8170 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 23/40\n",
      "28709/28709 [==============================] - 379s 13ms/step - loss: 1.8168 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 24/40\n",
      "28709/28709 [==============================] - 380s 13ms/step - loss: 1.8169 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 25/40\n",
      "28709/28709 [==============================] - 382s 13ms/step - loss: 1.8170 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 26/40\n",
      "28709/28709 [==============================] - 386s 13ms/step - loss: 1.8167 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 27/40\n",
      "28709/28709 [==============================] - 382s 13ms/step - loss: 1.8169 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 28/40\n",
      "28709/28709 [==============================] - 384s 13ms/step - loss: 1.8170 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 29/40\n",
      "28709/28709 [==============================] - 355s 12ms/step - loss: 1.8171 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 30/40\n",
      "28709/28709 [==============================] - 349s 12ms/step - loss: 1.8168 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 31/40\n",
      "28709/28709 [==============================] - 349s 12ms/step - loss: 1.8168 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 32/40\n",
      "28709/28709 [==============================] - 348s 12ms/step - loss: 1.8167 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 33/40\n",
      "28709/28709 [==============================] - 353s 12ms/step - loss: 1.8168 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 34/40\n",
      "28709/28709 [==============================] - 350s 12ms/step - loss: 1.8169 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 35/40\n",
      "28709/28709 [==============================] - 351s 12ms/step - loss: 1.8168 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 36/40\n",
      "28709/28709 [==============================] - 349s 12ms/step - loss: 1.8170 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 37/40\n",
      "28709/28709 [==============================] - 349s 12ms/step - loss: 1.8169 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709/28709 [==============================] - 349s 12ms/step - loss: 1.8168 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 39/40\n",
      "28709/28709 [==============================] - 350s 12ms/step - loss: 1.8167 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n",
      "Epoch 40/40\n",
      "28709/28709 [==============================] - 348s 12ms/step - loss: 1.8166 - acc: 0.2513 - val_loss: 1.8242 - val_acc: 0.2494\n"
     ]
    }
   ],
   "source": [
    "## Simple CNN with 3 layers definition:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D,Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import *\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2 #activity_l2\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "\n",
    "batch_size=64\n",
    "epochs=40\n",
    "num_class = 7\n",
    "num_output = 7\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Convolution2D(32, 3, 3, border_mode='same', activation='relu',#input_shape=(48,48,1)))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(48, 48, 1),  kernel_initializer= 'glorot_uniform'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer= 'glorot_uniform',kernel_regularizer=l2(1e-5)))\n",
    "#model.add(Convolution2D(32, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer= 'glorot_uniform',kernel_regularizer=l2(1e-5)))          \n",
    "#model.add(Convolution2D(32, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer= 'glorot_uniform',kernel_regularizer=l2(1e-5)))\n",
    "#model.add(Convolution2D(64, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer= 'glorot_uniform',kernel_regularizer=l2(1e-5)))\n",
    "#model.add(Convolution2D(64, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer= 'glorot_uniform',kernel_regularizer=l2(1e-5)))\n",
    "#model.add(Convolution2D(64, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer= 'glorot_uniform',kernel_regularizer=l2(1e-5)))\n",
    "#model.add(Convolution2D(128, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer= 'glorot_uniform',kernel_regularizer=l2(1e-5)))\n",
    "#model.add(Convolution2D(128, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer= 'glorot_uniform',kernel_regularizer=l2(1e-5)))\n",
    "#model.add(Convolution2D(128, 3, 3, border_mode='same', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64, activation='relu', kernel_initializer= 'glorot_uniform',kernel_regularizer=l2(1e-5)))\n",
    "model.add(Dense(64, activation='relu', kernel_initializer= 'glorot_uniform',kernel_regularizer=l2(1e-5)))\n",
    "model.add(Dense(7, activation='softmax',kernel_initializer= 'glorot_uniform',kernel_regularizer=l2(1e-5)))\n",
    "\n",
    "\n",
    "\n",
    "# Compiling the CNN\n",
    "sgd= SGD(lr=0.01,momentum=0.99, decay=0.9999, nesterov=True)\n",
    "\n",
    "#adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer = sgd , loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "print(\"Non-Training\")\n",
    "score = model.evaluate(x_train, to_categorical(y_train), verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "\n",
    "results = []\n",
    "start_time = time.time()\n",
    "history = model.fit(x_train, to_categorical(y_train), \n",
    "           batch_size=batch_size, \n",
    "           epochs=epochs, \n",
    "           verbose=1, \n",
    "           validation_data=(x_vali, to_categorical(y_vali)),shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
